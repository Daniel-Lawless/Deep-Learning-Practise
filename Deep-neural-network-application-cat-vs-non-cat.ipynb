{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-13T23:31:13.368906Z",
     "start_time": "2025-09-13T23:31:12.214589Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import neural_network\n",
    "import neural_net_optimized\n",
    "from lr_utils import load_dataset\n",
    "from sklearn.metrics import accuracy_score"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T23:31:13.379898Z",
     "start_time": "2025-09-13T23:31:13.373509Z"
    }
   },
   "cell_type": "code",
   "source": "train_x_orig, train_y, test_x_orig, test_y, classes = load_dataset()",
   "id": "e2b77b834e003436",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T23:31:13.388642Z",
     "start_time": "2025-09-13T23:31:13.385143Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_x_orig_shape = train_x_orig.shape\n",
    "train_y_shape = train_y.shape\n",
    "test_x_orig_shape = test_x_orig.shape\n",
    "test_y_shape = test_y.shape\n",
    "\n",
    "print(f\"train_x_orig shape: {train_x_orig.shape} \\ntrain_y shape: {train_y.shape} \\ntest_x_orig shape: {test_x_orig.shape} \\ntest_y shape: {test_y.shape}\")"
   ],
   "id": "3076604d6f1d227f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x_orig shape: (209, 64, 64, 3) \n",
      "train_y shape: (1, 209) \n",
      "test_x_orig shape: (50, 64, 64, 3) \n",
      "test_y shape: (1, 50)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T23:31:13.412129Z",
     "start_time": "2025-09-13T23:31:13.408946Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Reshape the training array such that each column represents an image\n",
    "train_x_orig_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T\n",
    "test_x_orig_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
    "\n",
    "print(f\"shape of train_x_orig_flatten = {train_x_orig_flatten.shape}, \\nshape of test_x_orig_flatten = {test_x_orig_flatten.shape}\")"
   ],
   "id": "460dd8e86f98c1d4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train_x_orig_flatten = (12288, 209), \n",
      "shape of test_x_orig_flatten = (12288, 50)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T23:31:13.440059Z",
     "start_time": "2025-09-13T23:31:13.431487Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Since pixel values are between 0 and 255, dividing by 255 will rescale their values to be between 0 and 1\n",
    "train_X = train_x_orig_flatten/255.\n",
    "test_X = test_x_orig_flatten/255.\n",
    "\n",
    "print(f\"Shape of train_X: {train_X.shape} \\nShape of test_X: {test_X.shape}\")"
   ],
   "id": "2c6966512c8d034",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_X: (12288, 209) \n",
      "Shape of test_X: (12288, 50)\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T23:31:47.239745Z",
     "start_time": "2025-09-13T23:31:13.450490Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Let us build 2 NN's, a 2-layer NN and an L-layer NN, and compare their performances\n",
    "def neural_network_blueprint(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
    "\n",
    "    np.random.seed(1) # Define np seed for reproducible results\n",
    "\n",
    "    NN = neural_network.NeuralNet(layers_dims) # Create NN instance, which initializes parameters.\n",
    "\n",
    "    costs = [] # Collection of costs for plotting\n",
    "\n",
    "    # Repeat forward prop, back prop, and gradient descent num_iterations times.\n",
    "    for i in range(num_iterations):\n",
    "\n",
    "        # Extract prediction from forward prop and caches from each layer.\n",
    "        AL, caches = NN.forward_propagation(X)\n",
    "\n",
    "        # Use the prediction to compute the cost\n",
    "        cost = NN.compute_cost(AL, Y)\n",
    "\n",
    "        # Perform back prop, initializing it with AL and Y.\n",
    "        grads = NN.backward_propagation(AL, Y, caches)\n",
    "\n",
    "        # Update parameter using gradient descent.\n",
    "        NN.update_parameters(grads, learning_rate)\n",
    "\n",
    "        # Print cost per 500 iterations and for last iteration if print_cost is true\n",
    "        if (i % 500 == 0 or i == num_iterations - 1) and print_cost == True:\n",
    "            print(f\"Cost after iteration {i}: {cost}\")\n",
    "            costs.append(cost)\n",
    "\n",
    "    return costs, NN\n",
    "\n",
    "# Define structure for the 2-layer NN.\n",
    "layers_dims = [12288, 7, 1]\n",
    "costs, two_layer_NN = neural_network_blueprint(train_X, train_y, layers_dims, print_cost = True)"
   ],
   "id": "df7b2f48d1a40227",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.7074426632736273\n",
      "Cost after iteration 500: 0.3336897138438556\n",
      "Cost after iteration 1000: 0.12412486168658173\n",
      "Cost after iteration 1500: 0.05362667055856123\n",
      "Cost after iteration 2000: 0.03330274414524972\n",
      "Cost after iteration 2500: 0.02306424025583977\n",
      "Cost after iteration 2999: 0.016151946179173104\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T23:31:47.252995Z",
     "start_time": "2025-09-13T23:31:47.246015Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Predictions for train and test with the two layer NN.\n",
    "predictions_train = two_layer_NN.predict(train_X)\n",
    "predictions_test = two_layer_NN.predict(test_X)\n",
    "\n",
    "# Select the first row of each array to make them 1D. Calculate accuracy for 2 layer NN\n",
    "accuracy_train = accuracy_score(predictions_train[0], train_y[0])\n",
    "accuracy_test = accuracy_score(predictions_test[0], test_y[0])\n",
    "print(f\"2 layer NN: \\nAccuracy_train: {100 * accuracy_train:.2f}% \\nAccuracy_test: {100 * accuracy_test:.2f}%\")"
   ],
   "id": "b47d28ddbb30ea6a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 layer NN: \n",
      "Accuracy_train: 100.00% \n",
      "Accuracy_test: 74.00%\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T23:32:31.556625Z",
     "start_time": "2025-09-13T23:31:47.260542Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set structure for the 4 layer NN\n",
    "layers_dims = [12288, 20, 7, 5, 1]\n",
    "\n",
    "# Train model.\n",
    "costs, four_layer_NN = neural_network_blueprint(train_X, train_y, layers_dims, print_cost = True)"
   ],
   "id": "18d155af423c47d7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 1.208125010600323\n",
      "Cost after iteration 500: 0.647319447526937\n",
      "Cost after iteration 1000: 0.6374058448197395\n",
      "Cost after iteration 1500: 0.6243486119913269\n",
      "Cost after iteration 2000: 0.5391408405511141\n",
      "Cost after iteration 2500: 0.2998273840124648\n",
      "Cost after iteration 2999: 0.08237974441376025\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T23:32:31.578291Z",
     "start_time": "2025-09-13T23:32:31.564741Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Predictions for train and test with the four layer NN.\n",
    "predictions_train = four_layer_NN.predict(train_X)\n",
    "predictions_test = four_layer_NN.predict(test_X)\n",
    "\n",
    "# Select the first row of each array to make them 1D. Calculate accuracy for 4 layer NN\n",
    "accuracy_train = accuracy_score(predictions_train[0], train_y[0])\n",
    "accuracy_test = accuracy_score(predictions_test[0], test_y[0])\n",
    "\n",
    "print(f\"{len(layers_dims) - 1} layer NN: \\nAccuracy_train: {100 * accuracy_train:.2f}% \\nAccuracy_test: {100 * accuracy_test:.2f}%\")\n",
    "\n",
    "# We can observe there is a large difference between our training and test set error. This means our model\n",
    "# Currently has high variance (overfitting). To try to combat this, we can try and accumulate more data,\n",
    "# or try regularization."
   ],
   "id": "754ed230b45eb7b4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 layer NN: \n",
      "Accuracy_train: 100.00% \n",
      "Accuracy_test: 80.00%\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T23:32:31.591468Z",
     "start_time": "2025-09-13T23:32:31.584935Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to create an L-layer neural network with L2 and dropout regularization\n",
    "def neural_network_reg(X, Y, layers_dims,\n",
    "                       lamda=0, keep_prob=1, learning_rate = 0.0075,\n",
    "                       num_iterations = 3000, print_cost=False, reg=False):\n",
    "\n",
    "    np.random.seed(1) # Define np seed for reproducible results.\n",
    "\n",
    "    NN = neural_network.NeuralNet(layers_dims) # Create NN instance, which initializes parameters.\n",
    "\n",
    "    costs = [] # Collection of costs for plotting.\n",
    "\n",
    "    # Repeat forward prop, back prop, and gradient descent num_iterations times.\n",
    "    for i in range(num_iterations):\n",
    "\n",
    "        # Extract prediction from forward prop and caches from each layer.\n",
    "        AL, caches = NN.forward_propagation_with_dropout(X, keep_prob)\n",
    "\n",
    "        # Use the prediction to compute the cost.\n",
    "        cost = NN.compute_cost_reg(AL, Y, lamda)\n",
    "\n",
    "        # Perform back prop.\n",
    "        grads = NN.backward_propagation_with_dropout(AL, Y, caches, keep_prob, lamda, reg)\n",
    "\n",
    "        # Update parameter using gradient descent.\n",
    "        NN.update_parameters(grads, learning_rate)\n",
    "\n",
    "        # Print cost per 500 iterations and for last iteration if print_cost is true.\n",
    "        if (i % 500 == 0 or i == num_iterations - 1) and print_cost == True:\n",
    "            print(f\"Cost after iteration {i}: {cost}\")\n",
    "            costs.append(cost)\n",
    "\n",
    "    return costs, NN"
   ],
   "id": "f8782d8cf75486f6",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T23:33:17.432172Z",
     "start_time": "2025-09-13T23:32:31.600992Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set structure for the 4 layer NN.\n",
    "layers_dims = [12288, 20, 7, 5, 1]\n",
    "\n",
    "# Train model.\n",
    "costs, four_layer_NN_reg = neural_network_reg(train_X, train_y, layers_dims,\n",
    "                                              lamda=0.4, keep_prob=0.9,\n",
    "                                              print_cost = True, reg = True)"
   ],
   "id": "3ec7401bec905f6b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 1.3363956407504674\n",
      "Cost after iteration 500: 0.7133243064748825\n",
      "Cost after iteration 1000: 0.6114794757911202\n",
      "Cost after iteration 1500: 0.3263759060098015\n",
      "Cost after iteration 2000: 0.21516703390282282\n",
      "Cost after iteration 2500: 0.1541758841485032\n",
      "Cost after iteration 2999: 0.15980388508192295\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T23:33:17.447167Z",
     "start_time": "2025-09-13T23:33:17.439425Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Calculate predictions for model.\n",
    "predictions_train = four_layer_NN_reg.predict(train_X)\n",
    "predictions_test = four_layer_NN_reg.predict(test_X)\n",
    "\n",
    "# Calculate accuracy for model.\n",
    "accuracy_train = accuracy_score(predictions_train[0], train_y[0])\n",
    "accuracy_test = accuracy_score(predictions_test[0], test_y[0])\n",
    "\n",
    "print(f\"{len(layers_dims) - 1} layer NN with reg: \\nAccuracy_train: {100 * accuracy_train:.2f}% \\nAccuracy_test: {100 * accuracy_test:.2f}%\")"
   ],
   "id": "3a512c52e4de4454",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 layer NN with reg: \n",
      "Accuracy_train: 100.00% \n",
      "Accuracy_test: 84.00%\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T23:33:17.481546Z",
     "start_time": "2025-09-13T23:33:17.479218Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# By using dropout and l2 regularization we have managed to increase the test_accuracy slightly.\n",
    "# To further improve on this we could do a hyperparameter search over the learning rate, keep_prob, and lamda value\n",
    "# to find a more optimal configuration of these parameters. We can also see at iteration 2999 the cost has increased\n",
    "# slightly this could mean our learning rate is too large at the end, to combat this we could implement learning rate decay\n",
    "# or use the Adam optimizer which adapts the learning rate during training."
   ],
   "id": "910bc2bfec5b66ab",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T23:33:17.491559Z",
     "start_time": "2025-09-13T23:33:17.487420Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to create an L-layer neural network with L2 and dropout regularization that uses the Adam optimizer.\n",
    "def neural_network_reg_adam(X, Y, layers_dims,\n",
    "                            lamda=0, keep_prob=1, beta_1=0.9,\n",
    "                            beta_2=0.999, learning_rate = 0.0075, num_iterations = 2000,\n",
    "                            print_cost=False, reg=False):\n",
    "\n",
    "    np.random.seed(1) # Define np seed for reproducible results\n",
    "\n",
    "    NN = neural_network.NeuralNet(layers_dims) # Create NN instance, which initializes parameters.\n",
    "\n",
    "    costs = [] # Collection of costs for plotting\n",
    "\n",
    "    # Repeat forward prop, back prop, and gradient descent num_iterations times.\n",
    "    for i in range(num_iterations):\n",
    "        t = i + 1\n",
    "        # Extract prediction from forward prop and caches from each layer.\n",
    "        AL, caches = NN.forward_propagation_with_dropout(X, keep_prob)\n",
    "\n",
    "        # Use the prediction to compute the cost\n",
    "        cost = NN.compute_cost_reg(AL, Y, lamda)\n",
    "\n",
    "        # Perform back prop\n",
    "        grads = NN.backward_propagation_with_dropout(AL, Y, caches, keep_prob, lamda, reg)\n",
    "\n",
    "        # Update parameter using Adam optimization.\n",
    "        NN.update_parameters_adam(grads, learning_rate, t, beta_1, beta_2)\n",
    "\n",
    "        # Print cost per 500 iterations and for last iteration if print_cost is true\n",
    "        if (i % 500 == 0 or i == num_iterations - 1) and print_cost == True:\n",
    "            print(f\"Cost after iteration {i}: {cost}\")\n",
    "            costs.append(cost)\n",
    "\n",
    "    return costs, NN"
   ],
   "id": "5a59a3b65262e4a9",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T23:34:00.673010Z",
     "start_time": "2025-09-13T23:33:17.497385Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set structure for the 4 layer NN\n",
    "layers_dims = [12288, 20, 7, 5, 1]\n",
    "\n",
    "# Train model.\n",
    "costs, four_layer_NN_reg_adam = neural_network_reg_adam(train_X, train_y, layers_dims,\n",
    "                                                        lamda=0.0008, keep_prob=0.9,\n",
    "                                                        beta_1=0.9, beta_2=0.999, learning_rate = 0.00005,\n",
    "                                                        print_cost=True, reg=True)"
   ],
   "id": "d81396ddf85b4cef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 1.2661729845409995\n",
      "Cost after iteration 500: 0.5985790532733838\n",
      "Cost after iteration 1000: 0.4806575932217139\n",
      "Cost after iteration 1500: 0.4617297991324054\n",
      "Cost after iteration 1999: 0.4387438885154836\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T23:34:00.690819Z",
     "start_time": "2025-09-13T23:34:00.681922Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Calculate predictions for model.\n",
    "predictions_train = four_layer_NN_reg_adam.predict(train_X)\n",
    "predictions_test = four_layer_NN_reg_adam.predict(test_X)\n",
    "\n",
    "# Calculate accuracy for model.\n",
    "accuracy_train = accuracy_score(predictions_train[0], train_y[0])\n",
    "accuracy_test = accuracy_score(predictions_test[0], test_y[0])\n",
    "\n",
    "print(f\"{len(layers_dims) - 1} layer NN with reg and Adam: \\nAccuracy_train: {100 * accuracy_train:.2f}% \\nAccuracy_test: {100 * accuracy_test:.2f}%\")"
   ],
   "id": "46387482310dc57c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 layer NN with reg and Adam: \n",
      "Accuracy_train: 100.00% \n",
      "Accuracy_test: 86.00%\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T23:34:00.710494Z",
     "start_time": "2025-09-13T23:34:00.707915Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# By reducing the number of iterations (another form of regularization called early stopping)\n",
    "# and implementing Adam optimization we have managed to squeeze an extra 2% from the model.\n",
    "# Too further improve the model we could perform hyperparameter tuning to find a more optimal config\n",
    "# of hyperparameters, we could collect more data, or create our own using data augmentation. We could\n",
    "# also improve the speed of our algorithm by using Adam on mini-batches of our training data instead\n",
    "# of our entire dataset at once. We could also use batch-norm to further improve learning speed whilst\n",
    "# increasing the stability of our network by giving the NN the flexibility to change the mean and variance\n",
    "# of the neurons."
   ],
   "id": "17dfcbf2209e10d3",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T23:34:00.722128Z",
     "start_time": "2025-09-13T23:34:00.717282Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to create a L-layer neural network with L2 and dropout regularization that uses the Adam optimizer\n",
    "# and implements batch-norm\n",
    "def neural_network_reg_adam_batch_norm(X, Y, layers_dims,\n",
    "                                       lamda=0, keep_prob=1, beta_1=0.9, beta_2=0.999,\n",
    "                                       learning_rate = 0.0075, num_epochs = 155,\n",
    "                                       print_cost=False, reg=False):\n",
    "\n",
    "    np.random.seed(1) # Define np seed for reproducible results\n",
    "\n",
    "    NN = neural_net_optimized.NeuralNetOptimized(layers_dims) # Create NN instance, which initializes parameters.\n",
    "\n",
    "    costs = [] # Collection of costs for plotting\n",
    "\n",
    "    t = 0\n",
    "    # Repeat forward prop, back prop, and Adam optimization num_epochs times.\n",
    "    for i in range(num_epochs):\n",
    "        # Create mini_batches out of our X and Y datasets.\n",
    "        mini_batches = NN.create_mini_batches(X, Y, mini_batch_size=64)\n",
    "        epoch_cost = 0\n",
    "\n",
    "        # Iterate through each mini_batch\n",
    "        for mini_batch in mini_batches:\n",
    "            mini_batch_X, mini_batch_Y = mini_batch\n",
    "\n",
    "            # Update t to reflect minibatch t.\n",
    "            t = t + 1\n",
    "\n",
    "            # Extract prediction from forward prop and caches from each layer.\n",
    "            AL, caches = NN.forward_propagation_with_dropout(mini_batch_X, keep_prob)\n",
    "\n",
    "            # Use the prediction to compute the cost\n",
    "            cost = NN.compute_cost_reg(AL, mini_batch_Y, lamda)\n",
    "\n",
    "            # Perform back prop\n",
    "            grads = NN.backward_propagation_with_dropout(AL, mini_batch_Y, caches, keep_prob, lamda, reg)\n",
    "\n",
    "            # Update parameter using Adam optimization.\n",
    "            NN.update_parameters_adam(grads, learning_rate, t, beta_1, beta_2)\n",
    "\n",
    "            epoch_cost += cost\n",
    "\n",
    "        # Print cost per 100 epochs and for last epoch if print_cost is true.\n",
    "        if (i % 100 == 0 or i == num_epochs - 1) and print_cost == True:\n",
    "            avg_cost = epoch_cost / len(mini_batches)\n",
    "            print(f\"Cost after epoch {i}: {avg_cost}\")\n",
    "            costs.append(avg_cost)\n",
    "\n",
    "    return costs, NN"
   ],
   "id": "11f1ef73b36e7162",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T23:34:09.779892Z",
     "start_time": "2025-09-13T23:34:00.729862Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set structure for the 4 layer NN\n",
    "layers_dims = [12288, 20, 7, 5, 1]\n",
    "\n",
    "# Train NN with L2 reg, Adam, and batch-norm.\n",
    "costs, four_layer_NN_reg_batch = neural_network_reg_adam_batch_norm(train_X, train_y, layers_dims,\n",
    "                                                        lamda=2.71, keep_prob=1,\n",
    "                                                        beta_1=0.9, beta_2=0.999, learning_rate = 0.0005009,\n",
    "                                                        print_cost=True, reg=True)"
   ],
   "id": "ef10880b62d57d1e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 3.40554710288063\n",
      "Cost after epoch 100: 1.041952239860188\n",
      "Cost after epoch 154: 0.8400026582136659\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T23:34:09.793191Z",
     "start_time": "2025-09-13T23:34:09.785477Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Calculate predictions for model.\n",
    "predictions_train = four_layer_NN_reg_batch.predict(train_X)\n",
    "predictions_test = four_layer_NN_reg_batch.predict(test_X)\n",
    "\n",
    "# Calculate accuracy for model.\n",
    "accuracy_train = accuracy_score(predictions_train[0], train_y[0])\n",
    "accuracy_test = accuracy_score(predictions_test[0], test_y[0])\n",
    "\n",
    "print(f\"{len(layers_dims) - 1} layer NN with reg, Adam, and batch-norm: \\nAccuracy_train: {100 * accuracy_train:.2f}% \\nAccuracy_test: {100 * accuracy_test:.2f}%\")"
   ],
   "id": "cc71898e45a91946",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 layer NN with reg, Adam, and batch-norm: \n",
      "Accuracy_train: 94.74% \n",
      "Accuracy_test: 88.00%\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T23:34:09.812402Z",
     "start_time": "2025-09-13T23:34:09.809647Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Using batch-norm with mini-batches with adam and L2 regularization gave an additional 2% training accuracy.\n",
    "# Not only did it improve the performance of our model on the test set, it also sped up training drastically,\n",
    "# Without batch-norm, training took approximately 1 minute 30 seconds to give a good result, conversely, the batch-norm\n",
    "# implementation not only gave a better result, but also took only approximately 9 seconds to give a good result.\n",
    "# For training models on small datasets, this boost may not seem very significant, but for larger models, this\n",
    "# speed boost could be the difference between training a model for months to training a model in weeks.\n",
    "\n",
    "# To improve model performance, we could try: hyperparameter tuning to try and achieve an even more optimized configuration\n",
    "# of the hyperparameters, overfitting is still present (although at a much lower rate than at which we started), so we could try\n",
    "# increase the regularization or collect more data, we can also observe that our training accuracy also dropped, to improve this\n",
    "# we could increase the depth of our NN or add more neurons per layer.\n",
    "\n",
    "# In this notebook I learned how to implement He initialization and why it is important when using\n",
    "# ReLu as our primary activation function. I also learned how to generalize my code to enable and\n",
    "# account for deeper NNs. I have also learned how to implement L2 and dropout regularization\n",
    "# and how they can help reduce overfitting. I learned how to implement Adam optimization and learned how\n",
    "# having a non-fixed learning rate can help improve model performance. I have also learned how to implement\n",
    "# mini-batches of the training data and applied batch norm.\n",
    "\n",
    "# What I could do better next time:\n",
    "# Currently I am making decisions on which model is better using the test set.\n",
    "# I should make a new set, the validation set, and use this set to evaluate the performance of the model\n",
    "# and pick optimal hyperparameters, then at the end, train a model using all the training data and the\n",
    "# optimal hyperparameters and test it on the test set.\n",
    "\n",
    "# Although I kept an array of the costs in my implementation (I did for practise), I never used them for plotting.\n",
    "# To aid in diagnosing NN performance I could plot the costs against the number iterations to visually analyse how\n",
    "# the cost function converges, which could help choose the best number of iterations.\n",
    "\n",
    "# Currently I am using several function calls, i.e., neural_network_reg, neural_network_blueprint, etc. I Could\n",
    "# refactor these into a single function where the optimizer and regularization used are passed as arguments,\n",
    "# which would mke my code more reusable and less repetitive."
   ],
   "id": "525f599124d7e34a",
   "outputs": [],
   "execution_count": 21
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
